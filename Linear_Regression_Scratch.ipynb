{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "089915df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4149c52",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "* __init__ is initializing the class and setting the parameters [m,b] b  - bias term\n",
    "* _prepare_featues is modifying the input array for the 2D multiplication. Adding the column of 1s in front of the column of Xs for the multiplication with self.theta [m,b]\n",
    "* hypothesis is returning the our predicted values.\n",
    "* initialize parameters is setting the self.theta i.e. our parameters [m,b]\n",
    "* in generate_synthetic_data we are creating our own data with some noise in it.\n",
    "* the cost function is a way of computing error between predicted and original y column\n",
    "* in linear regression we often use cost function to understand the differences.\n",
    "* it is defined as sum of square of diff btw predicted and original y values divided by twice the number of samples.\n",
    "* for a good linear regression model we tend to minimize the cost function and our cost function depends on 2 things m,b.\n",
    "* we can minimize it by using gradient descent algorithim. this algo subsequntly minimizes the parameters (here m and b) until the cost function is reached to its minimum value.\n",
    "* all the parameters are minimized by formula t = t - (alpha)*(partial derivative of J(m,b) wrt t) where t can be m or b and alpha will be figured out by ourselves (for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0efae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        # theta will store our parameters: [theta_0 (bias), theta_1 (weight for x1)] [m,b]\n",
    "        self.theta = None\n",
    "\n",
    "    def _prepare_features(self, X):\n",
    "        \"\"\"\n",
    "        Prepares the feature matrix X for calculations.\n",
    "        Adds a column of ones for the bias term (theta_0).\n",
    "        For a single feature X (1D array), it converts it to a 2D array:\n",
    "        [[1, x1],\n",
    "         [1, x2],\n",
    "         ...]\n",
    "\n",
    "        for many rows of data we have to make the X array into 2d matrix for multiplying it with [m,b]\n",
    "        \"\"\"\n",
    "        if X.ndim == 1: # If X is a 1D array (e.g., np.array([1, 2, 3]))\n",
    "            X = X.reshape(-1, 1) # Reshape to a column vector: [[1], [2], [3]]\n",
    "\n",
    "        # Add a column of ones to the left of X for the bias term (theta_0)\n",
    "        # np.c_ concatenates arrays column-wise\n",
    "        # This transforms X into a design matrix where the first column is all ones.\n",
    "        '''this thing makes the X_b matrix as :\n",
    "        [[1,x1],\n",
    "        [1,x2],\n",
    "        ....]'''\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b\n",
    "\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, num_features):\n",
    "        \"\"\"\n",
    "        num_features tells how many variables is there in your eqn\n",
    "        Initializes theta with zeros.\n",
    "        num_features is the number of 'x' variables (excluding the bias term).\n",
    "        So, theta will have size (num_features + 1) because it includes theta_0.\n",
    "        +1 is adding the y-intersect to the self.theta\n",
    "        \"\"\"\n",
    "        # theta will be [theta_0, theta_1] for single variable LR\n",
    "        self.theta = np.zeros(num_features + 1)\n",
    "    \n",
    "    def cost_function(self, targets, predicted):\n",
    "        m = len(predicted)\n",
    "        sum_of_squared_errors = np.sum((predicted - targets)**2)\n",
    "        return sum_of_squared_errors / (2 * m)\n",
    "    \n",
    "    def fit(self, X, y, alpha, iterations):\n",
    "        # implementing everything from scratch\n",
    "        X_b = self._prepare_features(X)\n",
    "        m = len(y)\n",
    "        num_features = 1 if X.ndim == 1 else X.shape[1]\n",
    "        self.initialize_parameters(num_features=num_features)\n",
    "        cost_hist = []\n",
    "\n",
    "        for i in range(iterations):\n",
    "            # the predicted error\n",
    "            # NumPy correctly treats self.theta as a column vector of \n",
    "            # shape (2, 1) for the purpose of the dot product.\n",
    "            predicted = X_b.dot(self.theta)\n",
    "            errors = predicted - y\n",
    "\n",
    "            # derivative is just sum of errors in case of b and sum of \n",
    "            # errors multiplied by each sample in case of m\n",
    "            derivatives = (1/m) * X_b.T.dot(errors)\n",
    "\n",
    "            # updating self.theta\n",
    "            self.theta = self.theta - (alpha)*(derivatives)\n",
    "            curr_cost = self.cost_function(y, predicted)\n",
    "            cost_hist.append(curr_cost)\n",
    "        print(self.theta)\n",
    "        return cost_hist\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        X_mod = self._prepare_features(x)\n",
    "        predictions = X_mod.dot(self.theta)\n",
    "        if x.ndim == 0:\n",
    "            return predictions[0]\n",
    "        else:\n",
    "            return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6822ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(num_samples=100,num_features=1, bias=2.0, weights= None, noise_std=1.0, random_seed=None):\n",
    "    \"\"\"\n",
    "    Generates synthetic linear data: y = bias + weight * x + noise.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): Number of data points to generate.\n",
    "        bias (float): The true intercept (theta_0).\n",
    "        weight (float): The true slope (theta_1).\n",
    "        noise_std (float): Standard deviation of the random noise.\n",
    "        random_seed (int, optional): An integer seed for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y) where X is a NumPy array of features and y is a NumPy array of target values.\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # Generate random x values between 0 and 10\n",
    "    X = np.random.rand(num_samples, num_features) * 10\n",
    "    # This is generating an array of weights for our regression \n",
    "    if weights is None:\n",
    "        weights = np.random.rand(num_features) * 2\n",
    "    # Generate noise from a normal distribution\n",
    "    noise = np.random.randn(num_samples) * noise_std\n",
    "\n",
    "    # Generate y values based on the linear equation and add noise\n",
    "    y = bias + X.dot(weights) + noise\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def MSE(predicted, target):\n",
    "    total_squared_error = 0\n",
    "    for i in range(len(predicted)):\n",
    "        error = (predicted[i] - target[i])**2\n",
    "        total_squared_error += error\n",
    "    return total_squared_error/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7c0d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaler:\n",
    "    def __init__(self, dataset):\n",
    "        self.mean = np.mean(dataset, axis=0)\n",
    "        self.std = np.std(dataset, axis=0)\n",
    "        self.range = np.max(dataset, axis=0) - np.min(dataset, axis=0)\n",
    "    \n",
    "    def mean_normalization(self, dataset):\n",
    "        self.range[self.range==0] = 1\n",
    "        normalized = (dataset-self.mean)/self.range\n",
    "        return normalized\n",
    "    \n",
    "    def Z_score(self, dataset):\n",
    "        self.std[self.std == 0] = 1\n",
    "        normalized = (dataset-self.mean)/self.std\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "034b6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated Data ---\n",
      "First 5 features (X): [[3.74540119 9.50714306 7.31993942]\n",
      " [5.98658484 1.5601864  1.5599452 ]\n",
      " [0.58083612 8.66176146 6.01115012]\n",
      " [7.08072578 0.20584494 9.69909852]\n",
      " [8.32442641 2.12339111 1.81824967]]\n",
      "First 5 targets (y): [15.81980293 14.06701995  9.71442967 16.62189525 17.89052623]\n",
      "Shape of X: (50, 3)\n",
      "Shape of y: (50,)\n",
      "--------------------\n",
      "\n",
      "--- Model Initialization ---\n",
      "Initial theta (model parameters): None\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Generate some synthetic data ---\n",
    "features, targets = generate_synthetic_data(num_samples=50, num_features=3, random_seed=42)\n",
    "\n",
    "print(\"--- Generated Data ---\")\n",
    "print(\"First 5 features (X):\", features[:5])\n",
    "print(\"First 5 targets (y):\", targets[:5])\n",
    "print(\"Shape of X:\", features.shape) # Should be (50,)\n",
    "print(\"Shape of y:\", targets.shape) # Should be (50,)\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Instantiate the Linear Regression model ---\n",
    "model = LinearRegression()\n",
    "\n",
    "# --- Initialize parameters (theta) ---\n",
    "# For single variable linear regression, we have 1 feature (the 'x' variable).\n",
    "# So, we need 1 parameter for 'x' (theta_1) + 1 parameter for the bias term (theta_0).\n",
    "# Total parameters = 1 + 1 = 2.\n",
    "\n",
    "print(\"\\n--- Model Initialization ---\")\n",
    "print(\"Initial theta (model parameters):\", model.theta) # Should be [0. 0.]\n",
    "print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586049f5",
   "metadata": {},
   "source": [
    "for alpha = 0.01 after 996 iterations the minimum cost function is 0.4127 where b = 2.00397958 and m = 2.99253883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc827980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04889595,  0.47466268,  0.24411636],\n",
       "       [ 0.1794782 , -0.36080586, -0.35404014],\n",
       "       [-0.37136167,  0.38578718,  0.10820289],\n",
       "       [ 0.29096997, -0.50318863,  0.49118423],\n",
       "       [ 0.4177017 , -0.3015958 , -0.32721607]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureScaler = FeatureScaler(dataset=features)\n",
    "mean_nor = featureScaler.mean_normalization(dataset=features)\n",
    "mean_nor[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cfb4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.21589269e+126 -5.54259903e+126 -7.13430695e+126 -6.66957547e+126]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(106.11010684566384),\n",
       " np.float64(1844.2039143783443),\n",
       " np.float64(34185.86610961393),\n",
       " np.float64(634310.5102452328),\n",
       " np.float64(11769765.88465767),\n",
       " np.float64(218390706.11495525),\n",
       " np.float64(4052289838.0752215),\n",
       " np.float64(75191171193.77663),\n",
       " np.float64(1395189498312.8916),\n",
       " np.float64(25888062458811.37),\n",
       " np.float64(480358961045777.5),\n",
       " np.float64(8913171150761270.0),\n",
       " np.float64(1.6538594344072637e+17),\n",
       " np.float64(3.0687742695755346e+18),\n",
       " np.float64(5.694181332275074e+19),\n",
       " np.float64(1.056568460127069e+21),\n",
       " np.float64(1.9604871109527183e+22),\n",
       " np.float64(3.637728985161541e+23),\n",
       " np.float64(6.749889910295648e+24),\n",
       " np.float64(1.2524576181171381e+26),\n",
       " np.float64(2.3239639550075984e+27),\n",
       " np.float64(4.3121686403199615e+28),\n",
       " np.float64(8.001328223052459e+29),\n",
       " np.float64(1.4846648791607904e+31),\n",
       " np.float64(2.75482987569859e+32),\n",
       " np.float64(5.1116502791736105e+33),\n",
       " np.float64(9.484784816321798e+34),\n",
       " np.float64(1.7599236665007596e+36),\n",
       " np.float64(3.265578894925972e+37),\n",
       " np.float64(6.059356847100691e+38),\n",
       " np.float64(1.1243276179165272e+40),\n",
       " np.float64(2.086215788751096e+41),\n",
       " np.float64(3.871021442397302e+42),\n",
       " np.float64(7.182769437513593e+43),\n",
       " np.float64(1.332779411330995e+45),\n",
       " np.float64(2.47300289215838e+46),\n",
       " np.float64(4.588713820628542e+47),\n",
       " np.float64(8.514464174059233e+48),\n",
       " np.float64(1.5798784366423615e+50),\n",
       " np.float64(2.9315008244113003e+51),\n",
       " np.float64(5.43946729331144e+52),\n",
       " np.float64(1.0093056835809232e+54),\n",
       " np.float64(1.8727899405910235e+55),\n",
       " np.float64(3.4750048658551135e+56),\n",
       " np.float64(6.447951559322141e+57),\n",
       " np.float64(1.1964322617181137e+59),\n",
       " np.float64(2.2200076159232293e+60),\n",
       " np.float64(4.119275258993564e+61),\n",
       " np.float64(7.643410111590938e+62),\n",
       " np.float64(1.4182523492796246e+64),\n",
       " np.float64(2.6315998969974196e+65),\n",
       " np.float64(4.882994215658743e+66),\n",
       " np.float64(9.060508224430942e+67),\n",
       " np.float64(1.6811981677497432e+69),\n",
       " np.float64(3.119501918914281e+70),\n",
       " np.float64(5.788307653900824e+71),\n",
       " np.float64(1.0740338158813458e+73),\n",
       " np.float64(1.992894480788787e+74),\n",
       " np.float64(3.697861606246833e+75),\n",
       " np.float64(6.861467373597308e+76),\n",
       " np.float64(1.2731610734000454e+78),\n",
       " np.float64(2.3623796930937456e+79),\n",
       " np.float64(4.383449927068359e+80),\n",
       " np.float64(8.133592292250164e+81),\n",
       " np.float64(1.5092067818097743e+83),\n",
       " np.float64(2.800367941273445e+84),\n",
       " np.float64(5.196147208607306e+85),\n",
       " np.float64(9.641570814883521e+86),\n",
       " np.float64(1.7890156696182055e+88),\n",
       " np.float64(3.31955977671066e+89),\n",
       " np.float64(6.159519616453106e+90),\n",
       " np.float64(1.1429130504486634e+92),\n",
       " np.float64(2.1207014868442914e+93),\n",
       " np.float64(3.935010449428407e+94),\n",
       " np.float64(7.301502513751792e+95),\n",
       " np.float64(1.3548106070739342e+97),\n",
       " np.float64(2.5138822832465008e+98),\n",
       " np.float64(4.664566472260997e+99),\n",
       " np.float64(8.655210516079639e+100),\n",
       " np.float64(1.6059942445486056e+102),\n",
       " np.float64(2.979959307438658e+103),\n",
       " np.float64(5.529383124586614e+104),\n",
       " np.float64(1.0259897731537254e+106),\n",
       " np.float64(1.903747653034499e+107),\n",
       " np.float64(3.532447614262269e+108),\n",
       " np.float64(6.554537901920694e+109),\n",
       " np.float64(1.216209603059812e+111),\n",
       " np.float64(2.256704928262695e+112),\n",
       " np.float64(4.187367967192971e+113),\n",
       " np.float64(7.76975769985677e+114),\n",
       " np.float64(1.4416964352658126e+116),\n",
       " np.float64(2.675100938471306e+117),\n",
       " np.float64(4.963711399959624e+118),\n",
       " np.float64(9.210280818849707e+119),\n",
       " np.float64(1.708988817576318e+121),\n",
       " np.float64(3.1710681096970786e+122),\n",
       " np.float64(5.883989908487948e+123),\n",
       " np.float64(1.0917878785799795e+125),\n",
       " np.float64(2.0258375530091455e+126),\n",
       " np.float64(3.7589882354436104e+127),\n",
       " np.float64(6.974889241842226e+128),\n",
       " np.float64(1.2942067622679109e+130),\n",
       " np.float64(2.401430453478557e+131),\n",
       " np.float64(4.455909512316739e+132),\n",
       " np.float64(8.268042721451261e+133),\n",
       " np.float64(1.5341543685926607e+135),\n",
       " np.float64(2.8466587631018203e+136),\n",
       " np.float64(5.282040894605679e+137),\n",
       " np.float64(9.800948527418852e+138),\n",
       " np.float64(1.818588571232273e+140),\n",
       " np.float64(3.3744329767311096e+141),\n",
       " np.float64(6.261338102842417e+142),\n",
       " np.float64(1.1618057050901762e+144),\n",
       " np.float64(2.1557572426368794e+145),\n",
       " np.float64(4.000057211649389e+146),\n",
       " np.float64(7.422198279105329e+147),\n",
       " np.float64(1.3772059843023747e+149),\n",
       " np.float64(2.555437421468212e+150),\n",
       " np.float64(4.741672988262549e+151),\n",
       " np.float64(8.79828343231389e+152),\n",
       " np.float64(1.6325417536584215e+154),\n",
       " np.float64(3.0292188219914913e+155),\n",
       " np.float64(5.6207852883053805e+156),\n",
       " np.float64(1.0429496551345188e+158),\n",
       " np.float64(1.9352171046426088e+159),\n",
       " np.float64(3.590839906474955e+160),\n",
       " np.float64(6.662886144918779e+161),\n",
       " np.float64(1.236313869078369e+163),\n",
       " np.float64(2.2940088568692775e+164),\n",
       " np.float64(4.256586265846622e+165),\n",
       " np.float64(7.89819384713325e+166),\n",
       " np.float64(1.4655280581864588e+168),\n",
       " np.float64(2.719321063652209e+169),\n",
       " np.float64(5.045762860639652e+170),\n",
       " np.float64(9.362529193818892e+171),\n",
       " np.float64(1.7372388541858415e+173),\n",
       " np.float64(3.223486703235496e+174),\n",
       " np.float64(5.981253816019295e+175),\n",
       " np.float64(1.1098354206250239e+177),\n",
       " np.float64(2.059325182915711e+178),\n",
       " np.float64(3.821125303968523e+179),\n",
       " np.float64(7.090186003531314e+180),\n",
       " np.float64(1.3156003419322954e+182),\n",
       " np.float64(2.4411267332483716e+183),\n",
       " np.float64(4.5295668736505536e+184),\n",
       " np.float64(8.404715651764136e+185),\n",
       " np.float64(1.5595143455753461e+187),\n",
       " np.float64(2.893714784443432e+188),\n",
       " np.float64(5.369354425923705e+189),\n",
       " np.float64(9.962960795644397e+190),\n",
       " np.float64(1.848650320722145e+192),\n",
       " np.float64(3.4302132452434736e+193),\n",
       " np.float64(6.364839675708615e+194),\n",
       " np.float64(1.1810106603036895e+196),\n",
       " np.float64(2.1913924793332225e+197),\n",
       " np.float64(4.0661792140329644e+198),\n",
       " np.float64(7.544889177343756e+199),\n",
       " np.float64(1.3999715630324758e+201),\n",
       " np.float64(2.5976794771021423e+202),\n",
       " np.float64(4.8200540953424536e+203),\n",
       " np.float64(8.943721381648354e+204),\n",
       " np.float64(1.6595281001067445e+206),\n",
       " np.float64(3.0792926093325206e+207),\n",
       " np.float64(5.71369835393566e+208),\n",
       " np.float64(1.0601898884446593e+210),\n",
       " np.float64(1.9672067545989258e+211),\n",
       " np.float64(3.650197438703115e+212),\n",
       " np.float64(6.773025412995426e+213),\n",
       " np.float64(1.2567504639250542e+215),\n",
       " np.float64(2.3319294292701132e+216),\n",
       " np.float64(4.3269487612620623e+217),\n",
       " np.float64(8.02875307785252e+218),\n",
       " np.float64(1.4897536241294593e+220),\n",
       " np.float64(2.764272159183752e+221),\n",
       " np.float64(5.12917065363983e+222),\n",
       " np.float64(9.51729427464498e+223),\n",
       " np.float64(1.7659558713631867e+225),\n",
       " np.float64(3.2767717899722546e+226),\n",
       " np.float64(6.0801255216358415e+227),\n",
       " np.float64(1.1281812933075982e+229),\n",
       " np.float64(2.0933663721908876e+230),\n",
       " np.float64(3.884289514650582e+231),\n",
       " np.float64(7.207388651148473e+232),\n",
       " np.float64(1.3373475631199643e+234),\n",
       " np.float64(2.4814792030091433e+235),\n",
       " np.float64(4.604441810624905e+236),\n",
       " np.float64(8.543647821719277e+237),\n",
       " np.float64(1.5852935296767699e+239),\n",
       " np.float64(2.941548654248366e+240),\n",
       " np.float64(5.458111272979586e+241),\n",
       " np.float64(1.0127651169461654e+243),\n",
       " np.float64(1.879208998872341e+244),\n",
       " np.float64(3.4869155763295344e+245),\n",
       " np.float64(6.4700521569157746e+246),\n",
       " np.float64(1.2005330785001574e+248),\n",
       " np.float64(2.227616775905734e+249),\n",
       " np.float64(4.1333942305830456e+250),\n",
       " np.float64(7.669608188540686e+251),\n",
       " np.float64(1.4231134627928517e+253),\n",
       " np.float64(2.640619805074567e+254)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X=features, y=targets, alpha=0.0691, iterations=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
