# 📂 CORE_ML Projects

This repository contains my learning journey in **Machine Learning (ML)**, with projects and implementations organized into two categories: **Monthly** and **Weekly**.

---

## 📁 Folder Structure

### 🔹 Monthly
Focused on implementing ML algorithms using **scikit-learn** and working on small projects.

- **MiniProject1.ipynb**
  - Implemented **Linear Regression** (with and without Polynomial Features) using `scikit-learn`.
  - Used datasets from `sklearn.datasets`.
  - Evaluated model performance using:
    - **R² Score**
    - **Mean Squared Error (MSE)**
  - Applied **StandardScaler** and **MinMaxScaler** to compare scaling effects.

- **MiniProject2.ipynb**
  - Worked with the **Iris dataset**.
  - Implemented and compared 3 models:
    - Logistic Regression
    - K-Nearest Neighbors Classifier (KNN)
    - Multinomial Naive Bayes
  - Generated **Classification Reports** to evaluate precision, recall, F1-score.

---

### 🔹 Weekly
Focused on **learning algorithms from scratch** and practicing with datasets.

- **Linear_Regression_Scratch**
  - Linear Regression implemented **from scratch** (without libraries like sklearn).

- **scratch_logistic_regression**
  - Logistic Regression implemented **from scratch**.

- **titanic.ipynb**
  - Used Titanic dataset to apply:
    - Logistic Regression
    - K-Nearest Neighbors Classifier
    - Decision Tree Classifier
    - Naive Bayes Classifier
  - Evaluated models using **Classification Reports**.

- **Spam_Classifier.ipynb**
  - (Planned / Ongoing) – Spam classifier implementation using ML algorithms.

- **data/**
  - Contains datasets used for practice and model training.

- **notes.md**
  - Notes and observations while learning ML.

---

## ⚡ Skills Practiced
- Implementing ML algorithms from scratch.
- Using `scikit-learn` for standard models.
- Data preprocessing with scaling (StandardScaler, MinMaxScaler).
- Model evaluation: R² Score, MSE, Classification Reports.
- Hands-on with datasets: Iris, Titanic, and synthetic data from sklearn.

---

## 🚀 Next Steps
- Add more ML models (SVM, Random Forest, Gradient Boosting).
- Explore hyperparameter tuning.
- Work on real-world datasets for better generalization.
